{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d025baf-19f9-4309-997e-446db6f1f449",
   "metadata": {},
   "source": [
    "In order to run this notebook you need the `mne_bids` package that you can install through this command:\n",
    "```bash\n",
    "pip install mne-bids\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d927cd-5281-456a-8005-e164685b1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne_bids\n",
    "from mne_bids import BIDSPath, read_raw_bids\n",
    "import shutil\n",
    "import platform\n",
    "\n",
    "# ERP CORE BIDS dataset\n",
    "# We'll use mne_bids to download and organize the first 5 participants' data\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Download the ERP CORE P3 dataset (if not already present)\n",
    "\n",
    "erp_core_url = \"https://osf.io/download/3zk6n/\"\n",
    "zip_path = \"sample_data/ERP_CORE_P3.zip\"\n",
    "extract_dir = \"sample_data/\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading ERP CORE dataset...\")\n",
    "    r = requests.get(erp_core_url, stream=True)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "print(\"Extracting ERP CORE dataset...\")\n",
    "with ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b5998-0bf3-414a-9255-4acee192f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmp\n",
    "\n",
    "sfreq = 250 #at what sampling rate we want the data, downsampling to 250Hz just to show that we can use any SF\n",
    "tmin, tmax = -.2, 2 #window size for the epochs, from 250ms before the stimulus up to 2 seconds after, data will be baseline corrected from tmin to 0\n",
    "\n",
    "epoch_data = hmp.io.read_mne_data([],\n",
    "                                  data_format='bids',\n",
    "                                  tmin=tmin, tmax=tmax,\n",
    "                                  sfreq=sfreq,\n",
    "                                  bids_parameters={\n",
    "                                      'bids_root': 'sample_data/ERP_CORE',\n",
    "                                      'task': 'P3',\n",
    "                                      'datatype': 'eeg',\n",
    "                                      'session': 'P3'\n",
    "                                  },\n",
    "                                  high_pass = 0.1,\n",
    "                                  low_pass = 40,\n",
    "                                  reject_threshold=1e-4,#Reject if more than 100microV between stimulus and response\n",
    "                                  reference='average',# VERY IMPORTANT, use the average as reference, other type of reference (except REST) can give weird topographies\n",
    "                                  verbose=False\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bbe349-ccee-4a38-a86b-462602110dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3566db-59f6-4e8c-9132-34c9ec451fb8",
   "metadata": {},
   "source": [
    "## Adding the frequent rare condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0a43b-4716-4ea5-acab-2aff9b89b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing stimulus as rare or frequent based on the trigger description in the BIDS file\n",
    "def classify(stim):\n",
    "    try:\n",
    "        target = stim.split(\"target \")[1].split(\",\")[0].strip()\n",
    "        stimulus = stim.split(\"stimulus \")[1].strip()\n",
    "        return \"rare\" if target == stimulus else \"frequent\"\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "stimulus_type = xr.apply_ufunc(\n",
    "    classify,\n",
    "    epoch_data.coords[\"event_name\"],\n",
    "    vectorize=True,\n",
    "    output_dtypes=[str]\n",
    ")\n",
    "\n",
    "epoch_data = epoch_data.assign_coords(stimulus_type=((\"participant\", \"epoch\"), stimulus_type.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c5519-7415-4879-ab98-9972fcc1e484",
   "metadata": {},
   "source": [
    "## Correcting the montage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4a1a7-1819-40b2-8538-e17f27630d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "# Rename FP1 and FP2 channels to Fp1 and Fp2\n",
    "epoch_data = epoch_data.assign_coords(\n",
    "    channel=[\n",
    "        ch.replace(\"FP1\", \"Fp1\").replace(\"FP2\", \"Fp2\") if ch in [\"FP1\", \"FP2\"] else ch\n",
    "        for ch in epoch_data.channel.values\n",
    "    ]\n",
    ")\n",
    "\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "# Keep only channels in the montage that are in epoch_data.channel\n",
    "keep_chs = [ch for ch in montage.ch_names if ch in list(epoch_data.channel.values)]\n",
    "# Create a new montage with only the desired channels\n",
    "montage = mne.channels.make_dig_montage(\n",
    "    {ch: montage.get_positions()['ch_pos'][ch] for ch in keep_chs},\n",
    "    coord_frame=montage.get_positions()['coord_frame']\n",
    ")\n",
    "\n",
    "info = mne.create_info(list(epoch_data.channel.values), sfreq, ch_types='eeg')\n",
    "info = info.set_montage(montage, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd54ceb-4946-4019-a7ea-ad8175a7e57c",
   "metadata": {},
   "source": [
    "# Typical workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f180e-5194-4b84-b172-af50a5dbdd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the two next tutorials for more details on how to fit an HMP model and how to visualize the results.\n",
    "preprocessed = hmp.preprocessing.Standard(epoch_data, n_comp=10)\n",
    "event_properties = hmp.patterns.HalfSine.create_expected(sfreq=epoch_data.sfreq)\n",
    "\n",
    "# Just plotting expected pattern, FYI\n",
    "plt.plot(event_properties.template, 'x')\n",
    "plt.show()\n",
    "trial_data = hmp.trialdata.TrialData.from_preprocessed(preprocessed=preprocessed, pattern=event_properties.template)\n",
    "model = hmp.models.CumulativeMethod(event_properties)\n",
    "_, estimates = model.fit_transform(trial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751d8a6-5094-4ff1-8e7d-1d3a607abfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(epoch_data, estimates,info, as_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea3a07-38fb-4908-9148-3e4250ee2996",
   "metadata": {},
   "source": [
    "# Fitting per condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11f75d-f0f5-40aa-824c-169f8117070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_preprocessed_data = hmp.utils.condition_selection(preprocessed.data, 'frequent', variable='stimulus_type')\n",
    "trial_data_frequent = hmp.trialdata.TrialData.from_preprocessed(frequent_preprocessed_data, pattern=event_properties.template)\n",
    "\n",
    "rare_preprocesssed_data = hmp.utils.condition_selection(preprocessed.data, 'rare', variable='stimulus_type')\n",
    "trial_data_rare = hmp.trialdata.TrialData.from_preprocessed(rare_preprocesssed_data, pattern=event_properties.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794dfa9-0ef7-4a62-b77b-13037c926257",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_cumulative, estimates = model.fit_transform(trial_data_frequent)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, estimates, info, as_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ec4d8-e59f-4da8-8f0d-4e5236930253",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_cumulative, estimates = model.fit_transform(trial_data_rare)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, estimates, info, as_time=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmp",
   "language": "python",
   "name": "hmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
